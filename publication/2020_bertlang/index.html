<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.3.1"><meta name=description content="Recently, Natural Language Processing (NLP) has witnessed an impressive progress in many areas, due to the advent of novel, pretrained contextual representation models. In particular, Devlin et al. (2019) proposed a model, called BERT (Bidirectional Encoder Representations from Transformers), which enables researchers to obtain state-of-the art performance on numerous NLP tasks by fine-tuning the representations on their data set and task, without the need for developing and training highly-specific architectures. The authors also released multilingual BERT (mBERT), a model trained on a corpus of 104 languages, which can serve as a universal language model. This model obtained impressive results on a zero-shot cross-lingual natural inference task. Driven by the potential of BERT models, the NLP community has started to investigate and generate an abundant number of BERT models that are trained on a particular language, and tested on a specific data domain and task. This allows us to evaluate the true potential of mBERT as a universal language model, by comparing it to the performance of these more specific models. This paper presents the current state of the art in language-specific BERT models, providing an overall picture with respect to different dimensions (i.e. architectures, data domains, and tasks). Our aim is to provide an immediate and straightforward overview of the commonalities and differences between Language-Specific (language-specific) BERT models and mBERT. We also provide an interactive and constantly updated website that can be used to explore the information we have collected, at [https://bertlang.unibocconi.it](https://bertlang.unibocconi.it/)."><link rel=alternate hreflang=en-us href=https://deboranozza.com/publication/2020_bertlang/><meta name=theme-color content="#c02739"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/paraiso-light.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/paraiso-light.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href="//fonts.googleapis.com/css?family=Nunito"><link rel=stylesheet href=/css/academic.min.5b1fd6414ab0b9f89dc644df97eddd79.css><link rel=stylesheet href=/css/academic.8fa6fab8892f3fb614ebe2b828140a5f.css><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-159440078-1','auto');ga('set','anonymizeIp',true);ga('require','eventTracker');ga('require','outboundLinkTracker');ga('require','urlChangeTracker');ga('send','pageview');</script><script async src=//www.google-analytics.com/analytics.js></script><script async src=https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin=anonymous></script><link rel=manifest href=/site.webmanifest><link rel=icon type=image/png href=/img/icon.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=https://deboranozza.com/publication/2020_bertlang/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@debora_nozza"><meta property="twitter:creator" content="@debora_nozza"><meta property="og:site_name" content="Debora Nozza"><meta property="og:url" content="https://deboranozza.com/publication/2020_bertlang/"><meta property="og:title" content="What the [MASK]? Making Sense of Language-Specific BERT Models | Debora Nozza"><meta property="og:description" content="Recently, Natural Language Processing (NLP) has witnessed an impressive progress in many areas, due to the advent of novel, pretrained contextual representation models. In particular, Devlin et al. (2019) proposed a model, called BERT (Bidirectional Encoder Representations from Transformers), which enables researchers to obtain state-of-the art performance on numerous NLP tasks by fine-tuning the representations on their data set and task, without the need for developing and training highly-specific architectures. The authors also released multilingual BERT (mBERT), a model trained on a corpus of 104 languages, which can serve as a universal language model. This model obtained impressive results on a zero-shot cross-lingual natural inference task. Driven by the potential of BERT models, the NLP community has started to investigate and generate an abundant number of BERT models that are trained on a particular language, and tested on a specific data domain and task. This allows us to evaluate the true potential of mBERT as a universal language model, by comparing it to the performance of these more specific models. This paper presents the current state of the art in language-specific BERT models, providing an overall picture with respect to different dimensions (i.e. architectures, data domains, and tasks). Our aim is to provide an immediate and straightforward overview of the commonalities and differences between Language-Specific (language-specific) BERT models and mBERT. We also provide an interactive and constantly updated website that can be used to explore the information we have collected, at [https://bertlang.unibocconi.it](https://bertlang.unibocconi.it/)."><meta property="og:image" content="https://deboranozza.com/publication/2020_bertlang/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2020-02-29T14:48:20+01:00"><meta property="article:modified_time" content="2020-03-01T00:00:00+00:00"><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css><script src=//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js></script><script>window.addEventListener("load",function(){window.cookieconsent.initialise({"palette":{"popup":{"background":"#c02739","text":"#ffffee"},"button":{"background":"#ffffee","text":"#c02739"}},"theme":"classic","content":{"message":"This website uses cookies to ensure you get the best experience on our website.","dismiss":"Got it!","link":"Learn more","href":"https://cookies.insites.com"}})});</script><title>What the [MASK]? Making Sense of Language-Specific BERT Models | Debora Nozza</title></head><body id=top data-spy=scroll data-target=#TableOfContents data-offset=71><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off role=textbox spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id=navbar-main><div class=container><a class=navbar-brand href=/>Debora Nozza</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/projects><span>Projects</span></a></li><li class=nav-item><a class="nav-link active" href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>About / Contact</span></a></li><li class=nav-item><a class="nav-link js-search" href=#><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><div class=pub itemscope itemtype=http://schema.org/CreativeWork><div class="article-header d-xl-none"><div class=featured-image style=background-image:url(/publication/2020_bertlang/featured_hu4c24e3f89ed2c995a376e861c7b88cf1_163425_800x0_resize_lanczos_2.png)></div><span class=article-header-caption><a href=https://bertlang.unibocconi.it/>https://bertlang.unibocconi.it/</a></span></div><div class="container-fluid split-header d-none d-xl-block"><div class=row><div class=col-6><div class=split-header-content><h1 itemprop=name>What the [MASK]? Making Sense of Language-Specific BERT Models</h1><meta content="2020-03-01 00:00:00 +0000 UTC" itemprop=datePublished><meta content="2020-03-01 00:00:00 +0000 UTC" itemprop=dateModified><div class=article-metadata><div><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/debora-nozza/>Debora Nozza</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/federico-bianchi/>Federico Bianchi</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/dirk-hovy/>Dirk Hovy</a></span></div><span class=article-date><time>March 2020</time></span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary my-1 mr-1" href=/publication/2020_bertlang/2020_bertlang.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 js-cite-modal" data-filename=/publication/2020_bertlang/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1" href=https://github.com/MilaNLProc/bertlang target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://bertlang.unibocconi.it/ target=_blank rel=noopener>Project</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://arxiv.org/abs/2003.02912 target=_blank rel=noopener>Source Document</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://deboranozza.com/publication/2020_bertlang/&text=What%20the%20[MASK]?%20Making%20Sense%20of%20Language-Specific%20BERT%20Models" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://deboranozza.com/publication/2020_bertlang/&t=What%20the%20[MASK]?%20Making%20Sense%20of%20Language-Specific%20BERT%20Models" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook-f"></i></a></li><li><a href="mailto:?subject=What%20the%20[MASK]?%20Making%20Sense%20of%20Language-Specific%20BERT%20Models&body=https://deboranozza.com/publication/2020_bertlang/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://deboranozza.com/publication/2020_bertlang/&title=What%20the%20[MASK]?%20Making%20Sense%20of%20Language-Specific%20BERT%20Models" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=What%20the%20[MASK]?%20Making%20Sense%20of%20Language-Specific%20BERT%20Models%20https://deboranozza.com/publication/2020_bertlang/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://deboranozza.com/publication/2020_bertlang/&title=What%20the%20[MASK]?%20Making%20Sense%20of%20Language-Specific%20BERT%20Models" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div></div></div><div class=col-6><div class=split-header-image><img src=/publication/2020_bertlang/featured_hu4c24e3f89ed2c995a376e861c7b88cf1_163425_680x500_fill_q90_lanczos_left_2.png itemprop=image alt>
<span class=article-header-caption><a href=https://bertlang.unibocconi.it/>https://bertlang.unibocconi.it/</a></span></div></div></div></div><div class="article-container d-xl-none"><h1 itemprop=name>What the [MASK]? Making Sense of Language-Specific BERT Models</h1><meta content="2020-03-01 00:00:00 +0000 UTC" itemprop=datePublished><meta content="2020-03-01 00:00:00 +0000 UTC" itemprop=dateModified><div class=article-metadata><div><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/debora-nozza/>Debora Nozza</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/federico-bianchi/>Federico Bianchi</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/dirk-hovy/>Dirk Hovy</a></span></div><span class=article-date><time>March 2020</time></span><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://deboranozza.com/publication/2020_bertlang/&text=What%20the%20[MASK]?%20Making%20Sense%20of%20Language-Specific%20BERT%20Models" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://deboranozza.com/publication/2020_bertlang/&t=What%20the%20[MASK]?%20Making%20Sense%20of%20Language-Specific%20BERT%20Models" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook-f"></i></a></li><li><a href="mailto:?subject=What%20the%20[MASK]?%20Making%20Sense%20of%20Language-Specific%20BERT%20Models&body=https://deboranozza.com/publication/2020_bertlang/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://deboranozza.com/publication/2020_bertlang/&title=What%20the%20[MASK]?%20Making%20Sense%20of%20Language-Specific%20BERT%20Models" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=What%20the%20[MASK]?%20Making%20Sense%20of%20Language-Specific%20BERT%20Models%20https://deboranozza.com/publication/2020_bertlang/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://deboranozza.com/publication/2020_bertlang/&title=What%20the%20[MASK]?%20Making%20Sense%20of%20Language-Specific%20BERT%20Models" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div></div><div class="btn-links mb-3"><a class="btn btn-outline-primary my-1 mr-1" href=/publication/2020_bertlang/2020_bertlang.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 js-cite-modal" data-filename=/publication/2020_bertlang/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1" href=https://github.com/MilaNLProc/bertlang target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://bertlang.unibocconi.it/ target=_blank rel=noopener>Project</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://arxiv.org/abs/2003.02912 target=_blank rel=noopener>Source Document</a></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract itemprop=text>Recently, Natural Language Processing (NLP) has witnessed an impressive progress in many areas, due to the advent of novel, pretrained contextual representation models. In particular, Devlin et al. (2019) proposed a model, called BERT (Bidirectional Encoder Representations from Transformers), which enables researchers to obtain state-of-the art performance on numerous NLP tasks by fine-tuning the representations on their data set and task, without the need for developing and training highly-specific architectures. The authors also released multilingual BERT (mBERT), a model trained on a corpus of 104 languages, which can serve as a universal language model. This model obtained impressive results on a zero-shot cross-lingual natural inference task. Driven by the potential of BERT models, the NLP community has started to investigate and generate an abundant number of BERT models that are trained on a particular language, and tested on a specific data domain and task. This allows us to evaluate the true potential of mBERT as a universal language model, by comparing it to the performance of these more specific models. This paper presents the current state of the art in language-specific BERT models, providing an overall picture with respect to different dimensions (i.e. architectures, data domains, and tasks). Our aim is to provide an immediate and straightforward overview of the commonalities and differences between Language-Specific (language-specific) BERT models and mBERT. We also provide an interactive and constantly updated website that can be used to explore the information we have collected, at <a href=https://bertlang.unibocconi.it/>https://bertlang.unibocconi.it</a>.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#3>Preprint</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">arXiv preprint arXiv:2003.02912</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/tags/multilingual/>multilingual</a>
<a class="badge badge-light" href=/tags/bert/>BERT</a>
<a class="badge badge-light" href=/tags/representation-learning/>Representation learning</a>
<a class="badge badge-light" href=/tags/nlp/>NLP</a></div><div class="media author-card" itemscope itemtype=http://schema.org/Person><div class=media-body><h5 class=card-title itemprop=name><a href=/authors/debora-nozza/></a></h5><ul class=network-icon aria-hidden=true></ul></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/css.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/bash.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/javascript.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/markdown.min.js></script><script>hljs.initHighlightingOnLoad();</script><script>const search_index_filename="/index.json";const i18n={'placeholder':"Search...",'results':"results found",'no_results':"No results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/academic.min.3394a224b26ce58ff36f44c54743e0ab.js></script><div class=container><footer class=site-footer><p class=powered-by><a href=/license/>LICENSE: CC-BY-SA<br><i class="fab fa-creative-commons fa-2x"></i><i class="fab fa-creative-commons-by fa-2x"></i><i class="fab fa-creative-commons-sa fa-2x"></i></a><br></p><p class=powered-by>Â© Debora Nozza, 2023 &#183;
Made with <i class="far fa-heart" style=color:#e13d3d></i>and the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>. Inspired by <a href=https://alison.rbind.io target=_blank rel=noopener>Alison Hill's website</a>
<a href=https://github.com/rbind/apreshill target=_blank rel=noopener><i class="fas fa-code-branch" style=color:#e13d3d></i></a><span class=float-right aria-hidden=true><a href=# id=back_to_top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>